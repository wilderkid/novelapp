AI调用实现逻辑详解

  概述

  本项目实现了灵活的AI服务调用机制，支持多种AI提供商（如OpenAI、ChatAnywhere等
  ）。AI调用逻辑基于配置化的API端点，允许用户通过AI管理界面配置不同的AI服务提
  供商。

  架构设计

  前端架构
   - AI管理界面 (AIManagerView.vue): 提供用户界面用于配置AI提供商信息，包括：
     - 提供商名称（如OpenAI, ChatAnywhere等）
     - API Key
     - Base URL（API端点地址）
     - 模型列表管理

   - AI对话界面 (ChatView.vue): 实现对话功能，包括：
     - 项目ID验证
     - 提示模板选择
     - AI模型选择
     - 消息历史记录
     - 对话历史管理

  后端架构
   - API端点 `/api/ai-providers`: 管理AI提供商配置
   - API端点 `/api/ai-models`: 管理AI模型配置
   - API端点 `/api/chat`: 核心AI对话接口
   - 数据模型: AIProvider, AIModel 用于存储配置信息

  数据库模型

  AIProvider 模型

   1 class AIProvider(Base):
   2     id: int  # 主键
   3     project_id: Optional[int]  # 关联项目（可选，None表示全局）
   4     name: str  # 提供商名称
   5     api_key: str  # API密钥
   6     base_url: str  # API基础URL
   7     enabled: bool  # 是否启用
   8     is_system: bool  # 是否为系统提供商

  AIModel 模型

   1 class AIModel(Base):
   2     id: int  # 主键
   3     provider_id: int  # 关联的提供商ID
   4     name: str  # 显示名称
   5     model_identifier: str  # 模型标识符（用于API调用）
   6     temperature: float  # 温度参数
   7     max_tokens: int  # 最大token数

  核心调用逻辑

  1. 前端请求流程 (ChatView.vue)

   1 用户输入消息 → 前端验证 → 构建请求载荷 → 发送API请求 → 处理响应

  详细步骤：
   1. 验证用户是否已选择项目
   2. 收集上下文消息历史
   3. 构建包含以下信息的请求载荷：
      - 用户消息内容
      - 项目ID
      - 对话ID（新对话为None）
      - 消息历史
      - 选中的提示模板ID
      - 选中的AI模型ID
   4. 通过HTTP POST发送请求到 /api/chat

  2. 后端处理流程 (main.py)

  /api/chat 端点处理逻辑

   1 接收请求 → 验证参数 → 获取AI配置 → 构建API请求 → HTTP调用AI服务 → 
     处理响应 → 返回结果

  详细步骤：

   1. 参数验证与数据准备

   1    # 获取AI模型和提供商配置
   2    selected_ai_model = db.query(AIModel).filter(AIModel.id ==
     request.ai_model_id).first()
   3    selected_ai_provider = db.query(AIProvider).filter(AIProvider.id
     == selected_ai_model.provider_id).first()

   2. 消息历史构建
      - 包含系统提示词（如果存在）
      - 包含历史对话消息
      - 添加当前用户消息

   3. HTTP API请求构建

    1    # 使用配置的API密钥和基础URL
    2    base_url = selected_ai_provider.base_url or
      "https://api.openai.com/v1"
    3    api_key = selected_ai_provider.api_key
    4 
    5    headers = {
    6        "Content-Type": "application/json",
    7        "Authorization": f"Bearer {api_key}"
    8    }
    9 
   10    payload = {
   11        "model": selected_ai_model.model_identifier,
   12        "messages": messages_for_ai,
   13        "temperature": selected_ai_model.temperature,
   14        "max_tokens": selected_ai_model.max_tokens,
   15    }

   4. AI服务调用

   1    # 使用requests库发起HTTP POST请求
   2    api_endpoint = f"{base_url.rstrip('/')}/chat/completions"
   3    response = requests.post(
   4        api_endpoint,
   5        json=payload,
   6        headers=headers,
   7        timeout=30.0
   8    )

   5. 响应处理
      - HTTP 200: 提取AI回复内容
      - HTTP 401: API密钥无效
      - HTTP 404: API端点错误
      - HTTP 429: 请求频率超限
      - 其他错误: 返回详细错误信息

  3. 对话历史管理

   - 新对话: 自动创建对话记录，使用消息前30字符作为标题
   - 现有对话: 追加用户和AI消息到现有对话
   - 消息存储: 在数据库中持久化所有对话消息

  错误处理

  前端错误处理
   - 项目未选择: 显示 "请先选择一个项目"
   - 网络错误: 显示 "抱歉，与AI连接时出现错误"
   - API错误: 显示具体错误信息

  后端错误处理
   - 认证失败: 返回 "API密钥无效或已过期"
   - 网络超时: 返回 "API请求超时"
   - API错误: 返回HTTP状态码和错误详情
   - 配置错误: 返回 "AI提供商配置不正确"

  配置灵活性

  支持的AI提供商类型
   1. OpenAI: 使用 https://api.openai.com/v1
   2. ChatAnywhere: 使用 https://api.chatanywhere.tech/v1/
   3. 其他OpenAI兼容API: 使用自定义URL

  配置参数
   - Base URL: API服务端点地址
   - API Key: 认证密钥
   - 模型标识符: 在API调用中使用的模型名称
   - 温度参数: 控制输出的随机性
   - 最大Token数: 控制响应长度

  安全性

   1. API密钥保护: 密钥仅在内存中处理，不记录到日志
   2. 输入验证: 验证所有传入参数
   3. 超时控制: 30秒请求超时防止长时间等待
   4. 错误信息: 避免泄露敏感配置信息

  扩展性

  此架构设计支持：
   - 添加新的AI提供商
   - 支持不同的API格式
   - 扩展对话功能（如流式响应）
   - 集成更多AI服务

  通过这种配置化的架构，系统能够灵活适应不同的AI服务提供商，为用户提供多样化的
  AI服务选择。